# The Chunk Size Problem

In 1956, George Miller published "The Magical Number Seven" and established something odd: human working memory doesn't scale linearly. We can hold about seven items, but what counts as an "item" is fluid. A chess master sees board positions as single chunks where a novice sees 32 pieces. Expertise is, in part, the ability to perceive at a larger grain.

Chase and Simon later showed this wasn't better memory—it was better *compression*. Masters had learned which patterns mattered enough to name.

There's a parallel here to what you're circling: the "right" chunk size for semantic analysis might not be arbitrary. If embeddings capture something like meaning, and human cognition requires meaningful units to reason about, then analyzable chunks and comprehensible chunks might converge on similar scales.

Shannon proved that optimal compression preserves signal and discards noise. Perhaps the middle layer you're looking for—between conversation and code—is exactly this: the compression that preserves intent while discarding incidental phrasing. The question is whether that boundary is discoverable, or whether it only exists in the act of a mind drawing it.
