# The Cache and the Oracle

In 1956, IBM's first disk drive—the RAMAC—weighed a ton and stored 5MB. Engineers immediately faced a question that still echoes: what do you compute once and store, versus compute on demand?

This is the precomputation tradeoff. Your embedding worker downloads a 27MB model, runs inference on 571 chunks, produces vectors. Every visitor pays this cost. The alternative: run it once at build time, ship the results.

But you're hesitating, and correctly so. Pre-computed embeddings handle known queries against known documents. A live model can embed *new* queries—the user's search terms, typed in real time.

This is the difference between a cache and an oracle. A cache answers questions you anticipated. An oracle answers questions you didn't.

The interesting architectures keep both. Static embeddings for the corpus (computed once, shipped as artifact). Live model for the query (loaded lazily, only when search activates). The corpus is frozen knowledge; the query is live intent. Different computational economies for different epistemic statuses.

You're not choosing between precomputation and live inference. You're choosing when each applies.
